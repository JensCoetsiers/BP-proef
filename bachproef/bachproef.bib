% Encoding: UTF-8

@book{Knuth1998,
 author = {Knuth, Donald E.},
 title = {The art of computer programming,  volume 3: (2nd ed.) sorting and searching},
 year = {1998},
 publisher = {Addison Wesley Longman Publishing Co., Inc.},
 address = {Redwood City, CA, USA}
}

@book{Pollefliet2011,
  author = {Pollefliet, Leen},
  title = {Schrijven van verslag tot eindwerk: do's en don'ts},
  year = {2011},
  publisher = {Academia Press},
  address = {Gent}
}

@article{Creeger2009,
  author = {Creeger, Mache},
  journal = {Communications of the ACM},
  number = {8},
  pages = {50--56},
  title = {{CTO Roundtable: Cloud Computing}},
  volume = {52},
  year = {2009}
}

@Article{Ren2020,
  author       = {Yi Ren and Jinglin Liu and Xu Tan and Chen Zhang and Tao Qin and Tao Qin and Zhou Zhao and Tie-Yan Liu and Tie-Yan Liu},
  date         = {2020},
  journaltitle = {Annual Meeting of the Association for Computational Linguistics},
  title        = {SimulSpeech: End-to-End Simultaneous Speech to Text Translation},
  doi          = {10.18653/v1/2020.acl-main.350},
  url          = {https://aclanthology.org/2020.acl-main.350/},
  abstract     = {In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/SimuSpeech.pdf:PDF},
  mag_id       = {3034586846},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Ajami2016,
  author       = {Ajami, Sima},
  date         = {2016},
  journaltitle = {The National medical journal of India},
  title        = {Use of speech-to-text technology for documentation by healthcare providers},
  number       = {3},
  pages        = {148},
  url          = {https://www.nmji.in/content/141/2016/29/3/Images/NatlMedJIndia_2016_29_3_148_192803.pdf},
  urldate      = {2023-11-03},
  volume       = {29},
  publisher    = {Medknow Publications and Media Pvt. Ltd.},
}

@Article{Ning2019,
  author         = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
  date           = {2019},
  journaltitle   = {Applied Sciences},
  title          = {A Review of Deep Learning Based Speech Synthesis},
  doi            = {10.3390/app9194050},
  issn           = {2076-3417},
  number         = {19},
  url            = {https://www.mdpi.com/2076-3417/9/19/4050},
  urldate        = {2023-10-29},
  volume         = {9},
  abstract       = {Speech synthesis, also known as text-to-speech (TTS), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence (AI). For speech synthesis, deep learning based techniques can leverage a large scale of alt;text, speech and gt; pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis (SPSS) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
  article-number = {4050},
  file           = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/applsci-09-04050-v2.pdf:PDF},
}

@Article{Kambouri2023,
  author       = {M Kambouri and H Simon and G Brooks},
  date         = {2023},
  journaltitle = {Research in developmental disabilities},
  title        = {Using speech-to-text technology to empower young writers with special educational needs.},
  doi          = {10.1016/j.ridd.2023.104466},
  url          = {https://www.sciencedirect.com/science/article/pii/S0891422223000446?via=ihub},
  urldate      = {2023-10-29},
  abstract     = {This article reports the first group-based intervention study in the UK of using speech to-text (STT) technology to improve the writing of children with special educational needs and disabilities (SEND). Over a period of five years, thirty children took part in total from three settings; a mainstream school, a special school and a special unit of a different mainstream school. All children had Education, Health and Care Plans because of their difficulties in spoken and written communication. Children were trained to use the Dragon STT system, and used it on set tasks for 16-18 weeks. Handwritten text and self-esteem were assessed before and after the intervention, and screen-written text at the end. The results showed that this approach had boosted the quantity and quality of handwritten text, with post-test screen-written text significantly better than handwritten at post-test. The self-esteem instrument also showed positive and statistically significant results. The findings support the feasibility of using STT to support children with writing difficulties. All the data were gathered before the Covid-19 pandemic; the implications of this, and of the innovative research design, are discussed.Crown Copyright © 2023. Published by Elsevier Ltd. All rights reserved.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Using speech-to-text technology to empower young writers with special educational needs - ScienceDirect.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {36863156},
}

@Article{Barbiers2004,
  author    = {Barbiers, Sjef and Bennis, HJ},
  date      = {2004},
  title     = {Reflexieven in dialecten van het Nederlands: chaos of structuur?},
  pages     = {43--58},
  url       = {https://pure.knaw.nl/portal/nl/publications/reflexieven-in-dialecten-van-het-nederlands-chaos-of-structuur},
  urldate   = {2024-02-01},
  booktitle = {Taeldeman, man van de taal, schatbewaarder van de taal},
  isbn      = {9038206518},
  publisher = {Academia Press en Vakgroep Nederlandse Taalkunde Universiteit Gent},
}

@Article{Ghyselen2020,
  author       = {Ghyselen, Anne-Sophie and Breitbarth, Anne and Farasyn, Melissa and Van Keymeulen, Jacques and van Hessen, Arjan},
  date         = {2020},
  journaltitle = {Frontiers in artificial intelligence},
  title        = {Clearing the transcription hurdle in dialect corpus building: The corpus of southern Dutch dialects as case study},
  pages        = {10},
  url          = {https://www.frontiersin.org/articles/10.3389/frai.2020.00010/full},
  urldate      = {2024-02-14},
  volume       = {3},
  publisher    = {Frontiers Media SA},
}

@Article{Li2022,
  author       = {Li, Jinyu and others},
  date         = {2022},
  journaltitle = {APSIPA Transactions on Signal and Information Processing},
  title        = {Recent advances in end-to-end automatic speech recognition},
  number       = {1},
  url          = {https://www.researchgate.net/publication/355872363_Recent_Advances_in_End-to-End_Automatic_Speech_Recognition},
  urldate      = {2024-02-17},
  volume       = {11},
  abstract     = {Recently, the speech community is seeing a significant trend of moving
from deep neural network based hybrid modeling to end to end (E2E)
modeling for automatic speech recognition (ASR). While E2E models
achieve the state of the art results in most benchmarks in terms of ASR
accuracy, hybrid models are still used in a large proportion of commercial
ASR systems at the current time. There are lots of practical factors that
affect the production model deployment decision. Traditional hybrid
models, being optimized for production for decades, are usually good
at these factors. Without providing excellent solutions to all these
factors, it is hard for E2E models to be widely commercialized. In this
paper, we will overview the recent advances in E2E models, focusing on
technologies addressing those challenges from the industry’s perspective.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/recent advances.pdf:PDF},
  priority     = {prio1},
  publisher    = {Now Publishers, Inc.},
  ranking      = {rank4},
}

@Article{Wei2022,
  author       = {X. Wei and C. Cucchiarini and R. {van Hout} and H. Strik},
  date         = {2022},
  journaltitle = {Speech Communication},
  title        = {Automatic Speech Recognition and Pronunciation Error Detection of Dutch Non-native Speech: cumulating speech resources in a pluricentric language},
  doi          = {https://doi.org/10.1016/j.specom.2022.08.004},
  issn         = {0167-6393},
  pages        = {1-9},
  url          = {https://www.sciencedirect.com/science/article/pii/S016763932200108X},
  urldate      = {2024-02-17},
  volume       = {144},
  abstract     = {The shortage of large-scale learners’ speech corpora and precise manual annotations are two major challenges for automatic L2 speech recognition and error detection in L2 speech, especially for non-dominant varieties of pluricentric languages. In these cases, collecting and annotating large non-native (L2 learner) corpora for all language varieties is often unattainable. In this study, we investigated ways of addressing these problems through conventional and transfer learning Deep Neural Network (DNN) based Automatic Speech Recognition (ASR) and ASR-based pronunciation error detection (PED) by cumulating Netherlandic Dutch and Flemish Dutch speech resources. First, we show that for ASR the baseline system can be improved by combining the Netherlandic Dutch and Flemish Dutch datasets. Next, through the knowledge learned from models trained on the Netherlandic Dutch data, the Flemish Dutch learners' ASR model can be further improved. In order to evaluate the performance of the PED algorithms in the absence of learner speech data with pronunciation error annotations, we introduced plausible pronunciation errors in the native corpora based on knowledge from Flemish learner speech, in order to simulate non-native speech errors. For PED we found that the results are much better for a GOP classifier trained on Flemish Dutch data than for one trained on Netherlandic Dutch data. PED produced worse results when the Netherlandic Dutch data were merged with the Flemish Dutch data, while for ASR, lower WERs were attained. Whether adding Netherlandic Dutch data to Flemish Dutch data is beneficial, thus seems to depend on the specific task the data are used for. We discuss these results, compare them to those of related research and suggest avenues for future research.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Automatic speech recogn.pdf:PDF},
  keywords     = {Pluricentric languages, Automatic speech recognition, Transfer learning, Pronunciation error detection},
  priority     = {prio1},
  ranking      = {rank5},
}

@Article{VanDyck2021,
  author       = {Van Dyck, Bob and BabaAli, Bagher and Van Compernolle, Dirk},
  date         = {2021-12},
  journaltitle = {Computational Linguistics in the Netherlands Journal},
  title        = {A Hybrid ASR System for Southern Dutch},
  pages        = {27–34},
  url          = {https://clinjournal.org/clinj/article/view/119},
  urldate      = {2024-02-17},
  volume       = {11},
  abstractnote = {&amp;lt;p&amp;gt;Classical hybrid models for automatic speech recognition were recently outperformed by end-toend models on popular benchmarks such as LibriSpeech. However, in many real life situations, hybrid systems can prevail due to independent training, optimization and tuning of the acoustic and language models. In this work, we implemented a state-of-the-art hybrid system for Southern Dutch. For the acoustic model, we train a HMM-DNN on 155 hrs of the Corpus Gesproken Nederlands (CGN) with a rather standard Kaldi recipe. As reference, we reused language models developed during our N-Best 2008 evaluation. We further investigated the effect of language model order and size on WER for a variety of test sets (held out data from CGN, N-Best dev and test sets). Best results, 10.12% WER on the N-Best test set, are obtained with a 400k lexicon and a 4-gram language model (with 231M parameters). This new hybrid system outperforms our older HMM-GMM based N-Best system by over 40%. Pruning away 90% of the LM parameters yields a compact model suitable for small scale real-time apps while only taking a 10% relative hit on performance.&amp;lt;/p&amp;gt;},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/02_AHybrid_VanDyck.pdf:PDF},
  priority     = {prio1},
  ranking      = {rank4},
}

@Article{Bentum2022,
  author    = {Bentum, Martijn and ten Bosch, Louis and van den Heuvel, Henk and Wills, Simone and van der Niet, Domenique and Dijkstra, Jelske and Van de Velde, Hans},
  date      = {2022},
  title     = {A Speech Recognizer for {F}risian/{D}utch Council Meetings},
  editor    = {Calzolari, Nicoletta and B{\'e}chet, Fr{\'e}d{\'e}ric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Odijk, Jan and Piperidis, Stelios},
  pages     = {1009--1015},
  url       = {https://aclanthology.org/2022.lrec-1.107},
  urldate   = {2024-02-17},
  abstract  = {We developed a bilingual Frisian and Dutch speech recognizer for council meetings in Frysl (the Netherlands). During these meetings both Frisian and Dutch are spoken, and code switching between both languages shows up frequently. The new speech recognizer is based on an existing speech recognizer for Frisian and Dutch named FAME!, which was trained and tested on historical radio broadcasts. Adapting a speech recognizer for the council meeting domain is challenging because of acoustic background noise, speaker overlap and the jargon typically used in council meetings. To train the new recognizer, we used the radio broadcast materials utilized for the development of the FAME! recognizer and added newly created manually transcribed audio recordings of council meetings from eleven Frisian municipalities, the Frisian provincial council and the Frisian water board. The council meeting recordings consist of 49 hours of speech, with 26 hours of Frisian speech and 23 hours of Dutch speech. Furthermore, from the same sources, we obtained texts in the domain of council meetings containing 11 million words; 1.1 million Frisian words and 9.9 million Dutch words. We describe the methods used to train the new recognizer, report the observed word error rates, and perform an error analysis on remaining errors.},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  file      = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/2022.lrec-1.107.pdf:PDF},
  location  = {Marseille, France},
  month     = jun,
  priority  = {prio1},
  publisher = {European Language Resources Association},
  ranking   = {rank4},
  year      = {2022},
}

@Article{Arts2021,
  author       = {Floor Arts and Deniz Başkent and Terrin N. Tamati},
  date         = {2021},
  journaltitle = {Speech Communication},
  title        = {Development and structure of the VariaNTS corpus: A spoken Dutch corpus containing talker and linguistic variability},
  doi          = {https://doi.org/10.1016/j.specom.2020.12.006},
  issn         = {0167-6393},
  pages        = {64-72},
  url          = {https://www.sciencedirect.com/science/article/pii/S0167639320303083},
  urldate      = {2024-02-17},
  volume       = {127},
  abstract     = {Speech perception and spoken word recognition are not only affected by what is being said, but also by who is speaking. Currently, publicly available corpora of spoken Dutch do not offer a wide variety of linguistic materials produced by multiple talkers. The VariaNTS (Variatie in Nederlandse Taal en Sprekers) corpus is a Dutch spoken corpus that was developed to maximize both linguistic and talker variability. It contains 1000 items from 11 linguistic subcategories, recorded by 8 male and 8 female native speakers of standard Dutch. The corpus contains audio recordings, orthographic transcriptions, item-specific details such as word frequencies, neighborhood densities and phonotactic probabilities, and talker details. The VariaNTS corpus aims to provide new materials to be used for broad assessment of speech perception and word recognition in Dutch clinical and academic settings.},
  keywords     = {Spoken Dutch, Speech corpus, Talker variability, Linguistic variability},
  priority     = {prio1},
  ranking      = {rank4},
}

@Article{Pratap2020,
  author       = {Vineel Pratap and Qiantong Xu and Anuroop Sriram and Gabriel Synnaeve and Ronan Collobert},
  date         = {2020},
  journaltitle = {ArXiv},
  title        = {MLS: A Large-Scale Multilingual Dataset for Speech Research},
  url          = {https://www.openslr.org/94/},
  urldate      = {2024-02-17},
  volume       = {abs/2012.03411},
  abstract     = {Dataset},
  priority     = {prio2},
}

@Article{Feng2024,
  author       = {Siyuan Feng and Bence Mark Halpern and Olya Kudina and Odette Scharenborg},
  date         = {2024},
  journaltitle = {Computer Speech & Language},
  title        = {Towards inclusive automatic speech recognition},
  doi          = {https://doi.org/10.1016/j.csl.2023.101567},
  issn         = {0885-2308},
  pages        = {101567},
  url          = {https://www.sciencedirect.com/science/article/pii/S0885230823000864},
  urldate      = {2024-02-18},
  volume       = {84},
  abstract     = {Practice and recent evidence show that state-of-the-art (SotA) automatic speech recognition (ASR) systems do not perform equally well for all speaker groups. Many factors can cause this bias against different speaker groups. This paper, for the first time, systematically quantifies and finds speech recognition bias against gender, age, regional accents and non-native accents, and investigates the origin of this bias by investigating bias cross-lingually (i.e., Dutch and Mandarin) and for two different SotA ASR architectures (a hybrid DNN-HMM and an attention based end-to-end (E2E) model) through a phoneme error analysis. The results show that only a fraction of the bias can be explained by pronunciation differences between speaker groups, and that in order to mitigate bias, language- and architecture specific solutions need to be found.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Towards inclusive asr.pdf:PDF},
  keywords     = {Inclusive automatic speech recognition, Bias, Gender, Age, Accent},
  priority     = {prio1},
  ranking      = {rank4},
}

@Article{Herygers2023,
  author       = {Herygers, Aaricia and Verkhodanova, Vass and Coler, Matt and Scharenborg, Odette and Georges, Munir},
  date         = {2023},
  title        = {Bias in Flemish Automatic Speech Recognition},
  pages        = {158--165},
  url          = {https://www.essv.de/pdf/2023_158_165.pdf},
  urldate      = {2024-02-18},
  booktitle    = {Elektronische Sprachsignalverarbeitung 2023: Tagungsband der 34. Konferenz M{\"u}nchen, 1.-3. M{\"a}rz 2023},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Bias flemish asr.pdf:PDF},
  organization = {TUDpress},
  priority     = {prio1},
  ranking      = {rank4},
}

@Article{Cucchiarini2006,
  author    = {Cucchiarini, Catia and Van hamme, Hugo and van Herwijnen, Olga and Smits, Felix},
  date      = {2006},
  title     = {{JASMIN}-{CGN}: Extension of the Spoken {D}utch Corpus with Speech of Elderly People, Children and Non-natives in the Human-Machine Interaction Modality},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
  url       = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/254_pdf.pdf},
  urldate   = {2024-02-18},
  abstract  = {Large speech corpora (LSC) constitute an indispensable resource for conducting research in speech processing and for developing real-life speech applications. In 2004 the Spoken Dutch Corpus (CGN) became available, a corpus of standard Dutch as spoken by adult natives in the Netherlands and Flanders. Owing to budget constraints, CGN does not include speech of children, non-natives, elderly people and recordings of speech produced in human-machine interactions. Since such recordings would be extremely useful for conducting research and for developing HLT applications for these specific groups of speakers of Dutch, a new project, JASMIN-CGN, was started which aims at extending CGN in different ways: by collecting a corpus of contemporary Dutch as spoken by children of different age groups, non-natives with different mother tongues and elderly people in the Netherlands and Flanders and, in addition, by collecting speech material in a communication setting that was not envisaged in CGN: human-machine interaction. We expect that the knowledge gathered from these data can be generalized to developing appropriate systems also for other speaker groups (i.e. adult natives). One third of the data will be collected in Flanders and two thirds in the Netherlands.},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)},
  location  = {Genoa, Italy},
  month     = may,
  priority  = {prio1},
  publisher = {European Language Resources Association (ELRA)},
  year      = {2006},
}

@Article{Zhlebinkov2022,
  author   = {Zhlebinkov, Nikolay},
  date     = {20022},
  title    = {Improving Northern Regional Dutch Speech Recognition by Adapting Perturbation-based Data Augmentation},
  url      = {https://repository.tudelft.nl/islandora/object/uuid:081e1dc0-6bb3-454c-95cf-b0ac50d7d554?collection=education},
  urldate  = {2024-02-18},
  abstract = {Automatic speech recognition (ASR) does not perform equally well on every speaker. There is bias against many attributes, including accent. To train Dutch ASR, there exists CGN(Corpus Gesproken Nederlands) and as an extension, the JASMIN corpus with annotated accented data. This paper focuses on improving ASR performance for NRAD (Northern regional accented Dutch) speech, training on speakers from the region of Overijssel. To achieve this improvement, the corpus data is augmented using Vocal Tract Length Perturbation (VTLP), which entails randomly warping the frequency of each recording using a factor in the range 0.9, 1.1. The baseline and augmented ASR systems are trained using trigram GMM-HMM (Gaussian mixture model hidden Markov models) through the Kaldi toolkit on the DelftBlue supercomputer. This leads to improvements on word error rates (WER) for all speaker groups and styles, with an overall relative improvement of 14,64 percent and the biggest improvement observed for male speakers - from 25.15 percent WER to 19,68 percent WER. The impact of this augmentation on other accents and non-accented speech is not explored. This experiment can serve as a stepping stone for developing overall more robust and less biased Dutch ASR.},
  file     = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/RP_Paper_NZhlebinkov_v3.5.pdf:PDF},
  year     = {2022},
}

@InProceedings{Lopez2022,
  author    = {Lopez, Alianda and Liesenfeld, Andreas and Dingemanse, Mark},
  booktitle = {Proceedings of the 18th Conference on Natural Language Processing (KONVENS 2022)},
  date      = {2022},
  title     = {Evaluation of Automatic Speech Recognition for Conversational Speech in Dutch, English and German: What Goes Missing?},
  pages     = {135--143},
  url       = {https://aclanthology.org/2022.konvens-1.16.pdf},
  urldate   = {2024-02-01},
  abstract  = {As voice user interfaces and conversational
agents grow in importance, automatic speech
recognition (ASR) encounters increasingly
free-form and informal input data. Conversational speech is at once the most challenging
and the most ecologically relevant type of data
for speech recognition in this context. Here
we evaluate the performance of several ASR
engines on conversational speech in three languages, focusing on the fate of backchannels
and other interactionally relevant elements of
talk. We propose forms of error analysis based
on ngram salience scoring that can complement
default measures like word error rates (WER)
and are more informative of ASR’s ability to
live up to the task of accurately representing
real-world interaction.},
}

@Comment{jabref-meta: databaseType:biblatex;}
