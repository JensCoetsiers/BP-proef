% Encoding: UTF-8

@Article{Ren2020,
  author       = {Yi Ren and Jinglin Liu and Xu Tan and Chen Zhang and Tao Qin and Tao Qin and Zhou Zhao and Tie-Yan Liu and Tie-Yan Liu},
  date         = {2020},
  journaltitle = {Annual Meeting of the Association for Computational Linguistics},
  title        = {SimulSpeech: End-to-End Simultaneous Speech to Text Translation},
  doi          = {10.18653/v1/2020.acl-main.350},
  url          = {https://aclanthology.org/2020.acl-main.350/},
  abstract     = {In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/SimuSpeech.pdf:PDF},
  mag_id       = {3034586846},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Wang2020,
  author       = {Changhan Wang and Juan Pino and Anne Wu and Jiatao Gu},
  date         = {2020},
  journaltitle = {International Conference on Language Resources and Evaluation},
  title        = {CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus},
  doi          = {10.48550/arXiv.2002.01320},
  url          = {https://arxiv.org/abs/2002.01320},
  abstract     = {Spoken language translation has recently witnessed a resurgence in popularity, thanks to the development of end-to-end models and the creation of new corpora, such as Augmented LibriSpeech and MuST-C. Existing datasets involve language pairs with English as a source language, involve very specific domains or are low resource. We introduce CoVoST, a multilingual speech-to-text translation corpus from 11 languages into English, diversified with over 11,000 speakers and over 60 accents. We describe the dataset creation methodology and provide empirical evidence of the quality of the data. We also provide initial benchmarks, including, to our knowledge, the first end-to-end many-to-one multilingual models for spoken language translation. CoVoST is released under CC0 license and free to use. We also provide additional evaluation data derived from Tatoeba under CC licenses.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Covost.pdf:PDF},
  mag_id       = {3032433061},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Sutton1997,
  author       = {Sutton J},
  date         = {1997},
  journaltitle = {Radiology management},
  title        = {Speech-to-text: the next revelation for recording data.},
  url          = {https://europepmc.org/article/med/10175328},
  abstract     = {There are numerous tools available for communicating and storing information. Administrators may want to consider speech-to-text-technology. Programs are available that are accurate, easy to use and easily integrated into existing hospital information systems. Computerized voice recognition systems that provide direct speech-to-text are available today and are often referred to as the "listening typewriter." You speak, the computer listens and then types out what it thinks it heard. Such a system has a vocabulary or collection of words that may be spoken by a user. The program analyzes the phonetic structure of words in the vocabulary. A database table compares sounds made by the voice to words in the vocabulary. Two types of recognition are available--realtime and deferred server-based. With realtime transcription, dictation appears on the workstation screen in front of the dictating radiologist. With the deferred system, the dictated session goes to a queue on a server and waits there to be processed, on operation often known as batch processing. Failure of the system can be prevented by clustering or having a redundant server that takes over for the primary server if it should fail. Options to consider for a speech transcription system include security for electronic signatures and a provision for quality assurance or editing professionals who will review documents for errors in recognition. Some software systems offer a program that examines the final document for new or misspelled words, while other systems require error correction to be completed within the last 10 dictated words. The distributed voice model contains a profile of users' pronunciations and allows the computer to recognize a voice that, associated with login ID, differentiates it from others. This model allows a physician to dictate from different locations. Although the cost of the hardware is impressive and requires a thorough cost benefit analysis, such a system can be an excellent solution for large institutions with several locations.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Speech-to-text_ the next revelation for recording data. - Abstract - Europe PMC.pdf:PDF},
  mag_id       = {2417649160},
  pmcid        = {null},
  pmid         = {10175328},
}

@Article{ajami2016use,
  author    = {Ajami, Sima},
  title     = {Use of speech-to-text technology for documentation by healthcare providers},
  number    = {3},
  pages     = {148},
  url       = {https://www.nmji.in/content/141/2016/29/3/Images/NatlMedJIndia_2016_29_3_148_192803.pdf},
  urldate   = {2023-11-03},
  volume    = {29},
  journal   = {The National medical journal of India},
  publisher = {Medknow Publications and Media Pvt. Ltd.},
  year      = {2016},
}

@Article{Roepke2019,
  author       = {Willem Röpke and Roxana Radulescu and Kyriakos Efthymiadis and Ann Nowé},
  date         = {2019},
  journaltitle = {BNAIC/BENELEARN},
  title        = {Training a Speech-to-Text Model for Dutch on the Corpus Gesproken Nederlands.},
  url          = {https://researchportal.vub.be/en/publications/training-a-speech-to-text-model-for-dutch-on-the-corpus-gesproken},
  urldate      = {2023-10-29},
  abstract     = {Speech-to-text, also known as Speech Recognition, is a technology that is able to recognize and transcribe spoken language into text. In subsequent steps, this transcription can be used to complete a multitude of tasks, such as providing automatic subtitles or parsing voice commands. In recent years, Speech-to-Text models have dramatically improved thanks partially to advances in Deep Learning methods. Starting from the open-source project DeepSpeech, we train speech-to-text models for Dutch, using the Corpus Gesproken Nederlands (CGN). First, we contribute a pre-processing pipeline for this dataset, to make it suitable for the task at hand, obtaining a ready-to-use speech-to-text dataset for Dutch. Second, we investigate the performance of Dutch and Flemish models trained from scratch, establishing a baseline for the CGN dataset for this task. Finally, we investigate the issue of transferring speech-totext models between related languages. In this case, we analyse how a pre-trained English model can be transferred and fine-tuned for Dutch.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Training_a_Speech_to_Text_Model_for_Dutch_on_the_Corpus_Gesproken_Nederlands.pdf:PDF},
  mag_id       = {2990925923},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Arun2021,
  author       = {HP Arun and Jithin Kunjumon and R. Sambhunath and Ancy S Ansalem},
  date         = {2021},
  title        = {Malayalam Speech to Text Conversion Using Deep Learning},
  url          = {https://www.iosrjen.org/Papers/vol11_issue7/Ser-2/D1107022430.pdf},
  urldate      = {2023-10-29},
  abstract     = {In the current scenario, speech recognition for several languages is becoming more popular. Recognizing speech is a very difficult task in the Malayalam language. This project aims to establish a Formal Malayalam Speech to Text converter for the language of Malayalam. The system considers only isolated words with constrained vocabulary. The word which is spoken by the speaker is given as the input to the system is presented in the display as the output. We are using deep learning and feature extraction techniques for this project. The proposed system is taking around 5-10 isolated words for tutoring the machine. Since the system is depending on the speaker voice, at the beginning the words are stored in .wav (waveform audio) file for training procedure. Several samples are stored and trained for each word. The input audio word will be collated with these stored words. Pre-processing process includes the transformation of speech signal into digitized format. This digital signal is passed to the first order filters for the smoothening signals, which would help in the rise of signal’s energy at a higher frequency. MFCC is the systematic technique for feature extraction. Mel-frequency cepstral coefficients are obtained, after the completion of this phase. MFCC examines the frequencies with human perception sensitivity. Following the pre-processing, syllabification, and feature extraction procedure, HMM is used to identify the speech and training. The speech recognition system based on ANN was implemented using LSTM which is a common form of neural network.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Malayalam_Speech_to_Text_Conversion_Usin.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Kumar2022,
  author       = {Yogesh Kumar and Apeksha Koul and Chamkaur Singh},
  date         = {2022},
  journaltitle = {Multimedia Tools and Applications},
  title        = {A deep learning approaches in text-to-speech system: a systematic review and recent research perspective},
  doi          = {10.1007/s11042-022-13943-4},
  url          = {https://rdcu.be/dpJZE},
  urldate      = {2023-10-29},
  abstract     = {Text-to-speech systems (TTS) have come a long way in the last decade and are now a popular research topic for creating various human-computer interaction systems. Although, a range of speech synthesis models for various languages with several motive applications is available based on domain requirements. However, recent developments in speech synthesis have primarily attributed to deep learning-based techniques that have improved a variety of application scenarios, including intelligent speech interaction, chatbots, and conversational artificial intelligence (AI). Text-to-speech systems are discussed in this survey article as an active topic of study that has achieved significant progress in the recent decade, particularly for Indian and non-Indian languages. Furthermore, the study also covers the lifecycle of text-to-speech systems as well as developed platforms in it. We performed an efficient search for published survey articles up to May 2021 in the web of science, PubMed, Scopus, EBSCO(Elton B. Stephens CO (company)) and Google Scholar for Text-to-speech Systems (TTS) in various languages based on different approaches. This survey article offers a study of the contributions made by various researchers in Indian and non-Indian language text-to-speech systems and the techniques used to implement it with associated challenges in designing TTS systems. The work also compared different language text-to-speech systems based on the quality metrics such as recognition rate, accuracy, TTS score, precision, recall, and F1-score. Further, the study summarizes existing ideas and their shortcomings, emphasizing the scope of future research in Indian and non-Indian languages TTS, which may assist beginners in designing robust TTS systems.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/A deep learning approaches in text.pdf:PDF},
  mag_id       = {4297536219},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Khanam2022,
  author       = {Fahima Khanam and Farha Akhter Munmun and Nadia Afrin Ritu and A. K. Saha and M. Mridha},
  date         = {2022},
  journaltitle = {Journal of Advances in Information Technology},
  title        = {Text to Speech Synthesis: A Systematic Review, Deep Learning Based Architecture and Future Research Direction},
  doi          = {10.12720/jait.13.5.398-412},
  url          = {http://www.jait.us/index.php?m=content&c=index&a=show&catid=221&id=1247},
  urldate      = {2023-10-29},
  abstract     = {—Text to Speech (TTS) synthesis is a process of translating natural language text into speech. Pieces of recorded speech generate synthesized speech and a database is maintained for storing this synthesized speech. A speech synthesizer’s output is determined through its resemblance to the person utter and its capacity to be implied. In recent years between the two main subsections: machine learning and deep learning of Artificial Intelligence (AI), deep learning has achieved huge success in the domain of text to speech synthesis. In this literature, a taxonomy is introduced which represents some of the deep learning-based architectures and models popularly used in speech synthesis. Different datasets that are used in TTS have also been discussed. Further, for evaluating the quality of the synthesized speech some of the widely used evaluation matrices are described. Finally, the paper concludes with the challenges and future directions of the text-to-speech synthesis system.},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Reddy2022,
  author       = {P Deepak Reddy},
  date         = {2022},
  journaltitle = {Machine learning and applications},
  title        = {Multilingual Speech to Text using Deep Learning based on MFCC Features},
  doi          = {10.5121/mlaij.2022.9202},
  url          = {https://aircconline.com/mlaij/V9N2/9222mlaij02.pdf},
  urldate      = {2023-10-29},
  abstract     = {The proposed methodology presented in the paper deals with solving the problem of multilingual speech recognition. Current text and speech recognition and translation methods have a very low accuracy in translating sentences which contain a mixture of two or more different languages. The paper proposes a novel approach to tackling this problem and highlights some of the drawbacks of current recognition and translation methods. The proposed approach deals with recognition of audio queries which contain a mixture of words in two different languages - Kannada and English. The novelty in the approach presented, is the use of a next Word Prediction model in combination with a Deep Learning speech recognition model to accurately recognise and convert the input audio query to text. Another method proposed to solve the problem of multilingual speech recognition and translation is the use of cosine similarity between the audio features of words for fast and accurate recognition. The dataset used for training and testing the models was generated manually by the authors as there was no pre-existing audio and text dataset which contained sentences in a mixture of both Kannada and English. The DL speech recognition model in combination with the Word Prediction model gives an accuracy of 71 percent when tested on the in-house multilingual dataset. This method outperforms other existing translation and recognition solutions for the same test set. Multilingual translation and recognition is an important problem to tackle as there is a tendency for people to speak in a mixture of languages. By solving this problem, the barrier of language and communication can be lifted and thus can help people connect better and more comfortably with each other.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/multilangual.pdf:PDF},
  mag_id       = {4285405669},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Do2022,
  author       = {P. Do and M. Coler and J. Dijkstra and E. Klabbers},
  date         = {2022},
  journaltitle = {SIGUL},
  title        = {Text-to-Speech for Under-Resourced Languages: Phoneme Mapping and Source Language Selection in Transfer Learning},
  url          = {http://www.lrec-conf.org/proceedings/lrec2022/workshops/SIGUL/pdf/2022.sigul-1.3.pdf},
  urldate      = {2023-10-29},
  abstract     = {We propose a new approach for phoneme mapping in cross-lingual transfer learning for text-to-speech (TTS) in under-resourced languages (URLs), using phonological features from the PHOIBLE database and a language-independent mapping rule. This approach was validated through our experiment, in which we pre-trained acoustic models in Dutch, Finnish, French, Japanese, and Spanish, and fine-tuned them with 30 minutes of Frisian training data. The experiment showed an improvement in both naturalness and pronunciation accuracy in the synthesized Frisian speech when our mapping approach was used. Since this improvement also depended on the source language, we then experimented on finding a good criterion for selecting source languages. As an alternative to the traditionally used language family criterion, we tested a novel idea of using Angular Similarity of Phoneme Frequencies (ASPF), which measures the similarity between the phoneme systems of two languages. ASPF was empirically confirmed to be more effective than language family as a criterion for source language selection, and also to affect the phoneme mapping’s effectiveness. Thus, a combination of our phoneme mapping approach and the ASPF measure can be beneficially adopted by other studies involving multilingual or cross-lingual TTS for URLs.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Text to speech for underresourced.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Roepke2019a,
  author       = {Willem Röpke and Roxana Radulescu and Kyriakos Efthymiadis and Ann Nowé},
  date         = {2019},
  journaltitle = {BNAIC/BENELEARN},
  title        = {DuStt - A Speech-to-Text Engine for Dutch.},
  url          = {https://biblio.vub.ac.be/vubirfiles/75754393/DuStt_a_Speech_to_Text_Engine_for_Dutch.pdf},
  urldate      = {2023-10-29},
  abstract     = {We develop and demonstrate a speech-to-text engine for Dutch, starting from the open-source project DeepSpeech and using the Corpus Gesproken Nederlands. The DuStt engine provides models targeted towards Dutch, Flemish or speakers from both Belgium and The Netherlands. Users can upload or record their own input as well as load prerecorded samples and obtain a transcription on the spot. The demonstration is video available at: https://youtu.be/DtTK0uo5W7s.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/DuStt_a_Speech_to_Text_Engine_for_Dutch.pdf:PDF},
  mag_id       = {2991588215},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Shakhovska2019,
  author       = {Nataliya Shakhovska and Oleh Basystiuk and Khrystyna Shakhovska},
  date         = {2019},
  journaltitle = {Modern Machine Learning Technologies},
  title        = {Development of the Speech-to-Text Chatbot Interface Based on Google API.},
  url          = {https://www.researchgate.net/profile/Oleh-Basystiuk/publication/360054074_Development_of_the_Speech-to-Text_Chatbot_Interface_Based_on_Google_API/links/626f7de4b277c02187dc7589/Development-of-the-Speech-to-Text-Chatbot-Interface-Based-on-Google-API.pdf},
  urldate      = {2023-10-29},
  abstract     = {The paper describes possibilities, which are provided by open APIs, and how to use them for creating unified interfaces using the example of our bot based on Google API. In last decade AI technologies became widespread and easy to implement and use. One of the most perspective technology in the AI field is speech recognition as part of natural language processing. New speech recognition technologies and methods will become a central part of future life because they save a lot of communication time, replacing common texting with voice/audio. In addition, this paper explores the advantages and disadvantages of well-known chatbots. The method of their improvement is built. The algorithms of Rabin-Karp and Knut-Pratt are used. The time complexity of proposed algorithm is compared with existed one.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/DevelopmentoftheSpeech-to-TextChatbotInterface.pdf:PDF},
  mag_id       = {2954056749},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Ning2019,
  author         = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
  date           = {2019},
  journaltitle   = {Applied Sciences},
  title          = {A Review of Deep Learning Based Speech Synthesis},
  doi            = {10.3390/app9194050},
  issn           = {2076-3417},
  number         = {19},
  url            = {https://www.mdpi.com/2076-3417/9/19/4050},
  urldate        = {2023-10-29},
  volume         = {9},
  abstract       = {Speech synthesis, also known as text-to-speech (TTS), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence (AI). For speech synthesis, deep learning based techniques can leverage a large scale of alt;text, speech and gt; pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis (SPSS) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
  article-number = {4050},
  file           = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/applsci-09-04050-v2.pdf:PDF},
}

@Article{Kambouri2023,
  author       = {M Kambouri and H Simon and G Brooks},
  date         = {2023},
  journaltitle = {Research in developmental disabilities},
  title        = {Using speech-to-text technology to empower young writers with special educational needs.},
  doi          = {10.1016/j.ridd.2023.104466},
  url          = {https://www.sciencedirect.com/science/article/pii/S0891422223000446?via=ihub},
  urldate      = {2023-10-29},
  abstract     = {This article reports the first group-based intervention study in the UK of using speech to-text (STT) technology to improve the writing of children with special educational needs and disabilities (SEND). Over a period of five years, thirty children took part in total from three settings; a mainstream school, a special school and a special unit of a different mainstream school. All children had Education, Health and Care Plans because of their difficulties in spoken and written communication. Children were trained to use the Dragon STT system, and used it on set tasks for 16-18 weeks. Handwritten text and self-esteem were assessed before and after the intervention, and screen-written text at the end. The results showed that this approach had boosted the quantity and quality of handwritten text, with post-test screen-written text significantly better than handwritten at post-test. The self-esteem instrument also showed positive and statistically significant results. The findings support the feasibility of using STT to support children with writing difficulties. All the data were gathered before the Covid-19 pandemic; the implications of this, and of the innovative research design, are discussed.Crown Copyright © 2023. Published by Elsevier Ltd. All rights reserved.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Using speech-to-text technology to empower young writers with special educational needs - ScienceDirect.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {36863156},
}

@Comment{jabref-meta: databaseType:biblatex;}
