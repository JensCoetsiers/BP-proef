% Encoding: UTF-8

@Article{Ren2020,
  author       = {Yi Ren and Jinglin Liu and Xu Tan and Chen Zhang and Tao Qin and Tao Qin and Zhou Zhao and Tie-Yan Liu and Tie-Yan Liu},
  date         = {2020},
  journaltitle = {Annual Meeting of the Association for Computational Linguistics},
  title        = {SimulSpeech: End-to-End Simultaneous Speech to Text Translation},
  doi          = {10.18653/v1/2020.acl-main.350},
  url          = {https://aclanthology.org/2020.acl-main.350/},
  abstract     = {In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/SimuSpeech.pdf:PDF},
  mag_id       = {3034586846},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Wang2020,
  author       = {Changhan Wang and Juan Pino and Anne Wu and Jiatao Gu},
  date         = {2020},
  journaltitle = {International Conference on Language Resources and Evaluation},
  title        = {CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus},
  doi          = {10.48550/arXiv.2002.01320},
  url          = {https://arxiv.org/abs/2002.01320},
  abstract     = {Spoken language translation has recently witnessed a resurgence in popularity, thanks to the development of end-to-end models and the creation of new corpora, such as Augmented LibriSpeech and MuST-C. Existing datasets involve language pairs with English as a source language, involve very specific domains or are low resource. We introduce CoVoST, a multilingual speech-to-text translation corpus from 11 languages into English, diversified with over 11,000 speakers and over 60 accents. We describe the dataset creation methodology and provide empirical evidence of the quality of the data. We also provide initial benchmarks, including, to our knowledge, the first end-to-end many-to-one multilingual models for spoken language translation. CoVoST is released under CC0 license and free to use. We also provide additional evaluation data derived from Tatoeba under CC licenses.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Covost.pdf:PDF},
  mag_id       = {3032433061},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Sutton1997,
  author       = {Sutton J},
  date         = {1997},
  journaltitle = {Radiology management},
  title        = {Speech-to-text: the next revelation for recording data.},
  url          = {https://europepmc.org/article/med/10175328},
  abstract     = {There are numerous tools available for communicating and storing information. Administrators may want to consider speech-to-text-technology. Programs are available that are accurate, easy to use and easily integrated into existing hospital information systems. Computerized voice recognition systems that provide direct speech-to-text are available today and are often referred to as the "listening typewriter." You speak, the computer listens and then types out what it thinks it heard. Such a system has a vocabulary or collection of words that may be spoken by a user. The program analyzes the phonetic structure of words in the vocabulary. A database table compares sounds made by the voice to words in the vocabulary. Two types of recognition are available--realtime and deferred server-based. With realtime transcription, dictation appears on the workstation screen in front of the dictating radiologist. With the deferred system, the dictated session goes to a queue on a server and waits there to be processed, on operation often known as batch processing. Failure of the system can be prevented by clustering or having a redundant server that takes over for the primary server if it should fail. Options to consider for a speech transcription system include security for electronic signatures and a provision for quality assurance or editing professionals who will review documents for errors in recognition. Some software systems offer a program that examines the final document for new or misspelled words, while other systems require error correction to be completed within the last 10 dictated words. The distributed voice model contains a profile of users' pronunciations and allows the computer to recognize a voice that, associated with login ID, differentiates it from others. This model allows a physician to dictate from different locations. Although the cost of the hardware is impressive and requires a thorough cost benefit analysis, such a system can be an excellent solution for large institutions with several locations.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Speech-to-text_ the next revelation for recording data. - Abstract - Europe PMC.pdf:PDF},
  mag_id       = {2417649160},
  pmcid        = {null},
  pmid         = {10175328},
}

@Article{ajami2016use,
  author    = {Ajami, Sima},
  title     = {Use of speech-to-text technology for documentation by healthcare providers},
  number    = {3},
  pages     = {148},
  url       = {https://www.nmji.in/content/141/2016/29/3/Images/NatlMedJIndia_2016_29_3_148_192803.pdf},
  urldate   = {2023-11-03},
  volume    = {29},
  journal   = {The National medical journal of India},
  publisher = {Medknow Publications and Media Pvt. Ltd.},
  year      = {2016},
}

@Article{Roepke2019,
  author       = {Willem Röpke and Roxana Radulescu and Kyriakos Efthymiadis and Ann Nowé},
  date         = {2019},
  journaltitle = {BNAIC/BENELEARN},
  title        = {Training a Speech-to-Text Model for Dutch on the Corpus Gesproken Nederlands.},
  url          = {https://researchportal.vub.be/en/publications/training-a-speech-to-text-model-for-dutch-on-the-corpus-gesproken},
  urldate      = {2023-10-29},
  abstract     = {Speech-to-text, also known as Speech Recognition, is a technology that is able to recognize and transcribe spoken language into text. In subsequent steps, this transcription can be used to complete a multitude of tasks, such as providing automatic subtitles or parsing voice commands. In recent years, Speech-to-Text models have dramatically improved thanks partially to advances in Deep Learning methods. Starting from the open-source project DeepSpeech, we train speech-to-text models for Dutch, using the Corpus Gesproken Nederlands (CGN). First, we contribute a pre-processing pipeline for this dataset, to make it suitable for the task at hand, obtaining a ready-to-use speech-to-text dataset for Dutch. Second, we investigate the performance of Dutch and Flemish models trained from scratch, establishing a baseline for the CGN dataset for this task. Finally, we investigate the issue of transferring speech-totext models between related languages. In this case, we analyse how a pre-trained English model can be transferred and fine-tuned for Dutch.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Training_a_Speech_to_Text_Model_for_Dutch_on_the_Corpus_Gesproken_Nederlands.pdf:PDF},
  mag_id       = {2990925923},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Arun2021,
  author       = {HP Arun and Jithin Kunjumon and R. Sambhunath and Ancy S Ansalem},
  date         = {2021},
  title        = {Malayalam Speech to Text Conversion Using Deep Learning},
  url          = {https://www.iosrjen.org/Papers/vol11_issue7/Ser-2/D1107022430.pdf},
  urldate      = {2023-10-29},
  abstract     = {In the current scenario, speech recognition for several languages is becoming more popular. Recognizing speech is a very difficult task in the Malayalam language. This project aims to establish a Formal Malayalam Speech to Text converter for the language of Malayalam. The system considers only isolated words with constrained vocabulary. The word which is spoken by the speaker is given as the input to the system is presented in the display as the output. We are using deep learning and feature extraction techniques for this project. The proposed system is taking around 5-10 isolated words for tutoring the machine. Since the system is depending on the speaker voice, at the beginning the words are stored in .wav (waveform audio) file for training procedure. Several samples are stored and trained for each word. The input audio word will be collated with these stored words. Pre-processing process includes the transformation of speech signal into digitized format. This digital signal is passed to the first order filters for the smoothening signals, which would help in the rise of signal’s energy at a higher frequency. MFCC is the systematic technique for feature extraction. Mel-frequency cepstral coefficients are obtained, after the completion of this phase. MFCC examines the frequencies with human perception sensitivity. Following the pre-processing, syllabification, and feature extraction procedure, HMM is used to identify the speech and training. The speech recognition system based on ANN was implemented using LSTM which is a common form of neural network.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Malayalam_Speech_to_Text_Conversion_Usin.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Kumar2022,
  author       = {Yogesh Kumar and Apeksha Koul and Chamkaur Singh},
  date         = {2022},
  journaltitle = {Multimedia Tools and Applications},
  title        = {A deep learning approaches in text-to-speech system: a systematic review and recent research perspective},
  doi          = {10.1007/s11042-022-13943-4},
  url          = {https://rdcu.be/dpJZE},
  urldate      = {2023-10-29},
  abstract     = {Text-to-speech systems (TTS) have come a long way in the last decade and are now a popular research topic for creating various human-computer interaction systems. Although, a range of speech synthesis models for various languages with several motive applications is available based on domain requirements. However, recent developments in speech synthesis have primarily attributed to deep learning-based techniques that have improved a variety of application scenarios, including intelligent speech interaction, chatbots, and conversational artificial intelligence (AI). Text-to-speech systems are discussed in this survey article as an active topic of study that has achieved significant progress in the recent decade, particularly for Indian and non-Indian languages. Furthermore, the study also covers the lifecycle of text-to-speech systems as well as developed platforms in it. We performed an efficient search for published survey articles up to May 2021 in the web of science, PubMed, Scopus, EBSCO(Elton B. Stephens CO (company)) and Google Scholar for Text-to-speech Systems (TTS) in various languages based on different approaches. This survey article offers a study of the contributions made by various researchers in Indian and non-Indian language text-to-speech systems and the techniques used to implement it with associated challenges in designing TTS systems. The work also compared different language text-to-speech systems based on the quality metrics such as recognition rate, accuracy, TTS score, precision, recall, and F1-score. Further, the study summarizes existing ideas and their shortcomings, emphasizing the scope of future research in Indian and non-Indian languages TTS, which may assist beginners in designing robust TTS systems.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/A deep learning approaches in text.pdf:PDF},
  mag_id       = {4297536219},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Khanam2022,
  author       = {Fahima Khanam and Farha Akhter Munmun and Nadia Afrin Ritu and A. K. Saha and M. Mridha},
  date         = {2022},
  journaltitle = {Journal of Advances in Information Technology},
  title        = {Text to Speech Synthesis: A Systematic Review, Deep Learning Based Architecture and Future Research Direction},
  doi          = {10.12720/jait.13.5.398-412},
  url          = {http://www.jait.us/index.php?m=content&c=index&a=show&catid=221&id=1247},
  urldate      = {2023-10-29},
  abstract     = {—Text to Speech (TTS) synthesis is a process of translating natural language text into speech. Pieces of recorded speech generate synthesized speech and a database is maintained for storing this synthesized speech. A speech synthesizer’s output is determined through its resemblance to the person utter and its capacity to be implied. In recent years between the two main subsections: machine learning and deep learning of Artificial Intelligence (AI), deep learning has achieved huge success in the domain of text to speech synthesis. In this literature, a taxonomy is introduced which represents some of the deep learning-based architectures and models popularly used in speech synthesis. Different datasets that are used in TTS have also been discussed. Further, for evaluating the quality of the synthesized speech some of the widely used evaluation matrices are described. Finally, the paper concludes with the challenges and future directions of the text-to-speech synthesis system.},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Reddy2022,
  author       = {P Deepak Reddy},
  date         = {2022},
  journaltitle = {Machine learning and applications},
  title        = {Multilingual Speech to Text using Deep Learning based on MFCC Features},
  doi          = {10.5121/mlaij.2022.9202},
  url          = {https://aircconline.com/mlaij/V9N2/9222mlaij02.pdf},
  urldate      = {2023-10-29},
  abstract     = {The proposed methodology presented in the paper deals with solving the problem of multilingual speech recognition. Current text and speech recognition and translation methods have a very low accuracy in translating sentences which contain a mixture of two or more different languages. The paper proposes a novel approach to tackling this problem and highlights some of the drawbacks of current recognition and translation methods. The proposed approach deals with recognition of audio queries which contain a mixture of words in two different languages - Kannada and English. The novelty in the approach presented, is the use of a next Word Prediction model in combination with a Deep Learning speech recognition model to accurately recognise and convert the input audio query to text. Another method proposed to solve the problem of multilingual speech recognition and translation is the use of cosine similarity between the audio features of words for fast and accurate recognition. The dataset used for training and testing the models was generated manually by the authors as there was no pre-existing audio and text dataset which contained sentences in a mixture of both Kannada and English. The DL speech recognition model in combination with the Word Prediction model gives an accuracy of 71 percent when tested on the in-house multilingual dataset. This method outperforms other existing translation and recognition solutions for the same test set. Multilingual translation and recognition is an important problem to tackle as there is a tendency for people to speak in a mixture of languages. By solving this problem, the barrier of language and communication can be lifted and thus can help people connect better and more comfortably with each other.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/multilangual.pdf:PDF},
  mag_id       = {4285405669},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Do2022,
  author       = {P. Do and M. Coler and J. Dijkstra and E. Klabbers},
  date         = {2022},
  journaltitle = {SIGUL},
  title        = {Text-to-Speech for Under-Resourced Languages: Phoneme Mapping and Source Language Selection in Transfer Learning},
  url          = {http://www.lrec-conf.org/proceedings/lrec2022/workshops/SIGUL/pdf/2022.sigul-1.3.pdf},
  urldate      = {2023-10-29},
  abstract     = {We propose a new approach for phoneme mapping in cross-lingual transfer learning for text-to-speech (TTS) in under-resourced languages (URLs), using phonological features from the PHOIBLE database and a language-independent mapping rule. This approach was validated through our experiment, in which we pre-trained acoustic models in Dutch, Finnish, French, Japanese, and Spanish, and fine-tuned them with 30 minutes of Frisian training data. The experiment showed an improvement in both naturalness and pronunciation accuracy in the synthesized Frisian speech when our mapping approach was used. Since this improvement also depended on the source language, we then experimented on finding a good criterion for selecting source languages. As an alternative to the traditionally used language family criterion, we tested a novel idea of using Angular Similarity of Phoneme Frequencies (ASPF), which measures the similarity between the phoneme systems of two languages. ASPF was empirically confirmed to be more effective than language family as a criterion for source language selection, and also to affect the phoneme mapping’s effectiveness. Thus, a combination of our phoneme mapping approach and the ASPF measure can be beneficially adopted by other studies involving multilingual or cross-lingual TTS for URLs.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Text to speech for underresourced.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Roepke2019a,
  author       = {Willem Röpke and Roxana Radulescu and Kyriakos Efthymiadis and Ann Nowé},
  date         = {2019},
  journaltitle = {BNAIC/BENELEARN},
  title        = {DuStt - A Speech-to-Text Engine for Dutch.},
  url          = {https://biblio.vub.ac.be/vubirfiles/75754393/DuStt_a_Speech_to_Text_Engine_for_Dutch.pdf},
  urldate      = {2023-10-29},
  abstract     = {We develop and demonstrate a speech-to-text engine for Dutch, starting from the open-source project DeepSpeech and using the Corpus Gesproken Nederlands. The DuStt engine provides models targeted towards Dutch, Flemish or speakers from both Belgium and The Netherlands. Users can upload or record their own input as well as load prerecorded samples and obtain a transcription on the spot. The demonstration is video available at: https://youtu.be/DtTK0uo5W7s.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/DuStt_a_Speech_to_Text_Engine_for_Dutch.pdf:PDF},
  mag_id       = {2991588215},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Shakhovska2019,
  author       = {Nataliya Shakhovska and Oleh Basystiuk and Khrystyna Shakhovska},
  date         = {2019},
  journaltitle = {Modern Machine Learning Technologies},
  title        = {Development of the Speech-to-Text Chatbot Interface Based on Google API.},
  url          = {https://www.researchgate.net/profile/Oleh-Basystiuk/publication/360054074_Development_of_the_Speech-to-Text_Chatbot_Interface_Based_on_Google_API/links/626f7de4b277c02187dc7589/Development-of-the-Speech-to-Text-Chatbot-Interface-Based-on-Google-API.pdf},
  urldate      = {2023-10-29},
  abstract     = {The paper describes possibilities, which are provided by open APIs, and how to use them for creating unified interfaces using the example of our bot based on Google API. In last decade AI technologies became widespread and easy to implement and use. One of the most perspective technology in the AI field is speech recognition as part of natural language processing. New speech recognition technologies and methods will become a central part of future life because they save a lot of communication time, replacing common texting with voice/audio. In addition, this paper explores the advantages and disadvantages of well-known chatbots. The method of their improvement is built. The algorithms of Rabin-Karp and Knut-Pratt are used. The time complexity of proposed algorithm is compared with existed one.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/DevelopmentoftheSpeech-to-TextChatbotInterface.pdf:PDF},
  mag_id       = {2954056749},
  pmcid        = {null},
  pmid         = {null},
}

@Article{Ning2019,
  author         = {Ning, Yishuang and He, Sheng and Wu, Zhiyong and Xing, Chunxiao and Zhang, Liang-Jie},
  date           = {2019},
  journaltitle   = {Applied Sciences},
  title          = {A Review of Deep Learning Based Speech Synthesis},
  doi            = {10.3390/app9194050},
  issn           = {2076-3417},
  number         = {19},
  url            = {https://www.mdpi.com/2076-3417/9/19/4050},
  urldate        = {2023-10-29},
  volume         = {9},
  abstract       = {Speech synthesis, also known as text-to-speech (TTS), has attracted increasingly more attention. Recent advances on speech synthesis are overwhelmingly contributed by deep learning or even end-to-end techniques which have been utilized to enhance a wide range of application scenarios such as intelligent speech interaction, chatbot or conversational artificial intelligence (AI). For speech synthesis, deep learning based techniques can leverage a large scale of alt;text, speech and gt; pairs to learn effective feature representations to bridge the gap between text and speech, thus better characterizing the properties of events. To better understand the research dynamics in the speech synthesis field, this paper firstly introduces the traditional speech synthesis methods and highlights the importance of the acoustic modeling from the composition of the statistical parametric speech synthesis (SPSS) system. It then gives an overview of the advances on deep learning based speech synthesis, including the end-to-end approaches which have achieved start-of-the-art performance in recent years. Finally, it discusses the problems of the deep learning methods for speech synthesis, and also points out some appealing research directions that can bring the speech synthesis research into a new frontier.},
  article-number = {4050},
  file           = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/applsci-09-04050-v2.pdf:PDF},
}

@Article{Kambouri2023,
  author       = {M Kambouri and H Simon and G Brooks},
  date         = {2023},
  journaltitle = {Research in developmental disabilities},
  title        = {Using speech-to-text technology to empower young writers with special educational needs.},
  doi          = {10.1016/j.ridd.2023.104466},
  url          = {https://www.sciencedirect.com/science/article/pii/S0891422223000446?via=ihub},
  urldate      = {2023-10-29},
  abstract     = {This article reports the first group-based intervention study in the UK of using speech to-text (STT) technology to improve the writing of children with special educational needs and disabilities (SEND). Over a period of five years, thirty children took part in total from three settings; a mainstream school, a special school and a special unit of a different mainstream school. All children had Education, Health and Care Plans because of their difficulties in spoken and written communication. Children were trained to use the Dragon STT system, and used it on set tasks for 16-18 weeks. Handwritten text and self-esteem were assessed before and after the intervention, and screen-written text at the end. The results showed that this approach had boosted the quantity and quality of handwritten text, with post-test screen-written text significantly better than handwritten at post-test. The self-esteem instrument also showed positive and statistically significant results. The findings support the feasibility of using STT to support children with writing difficulties. All the data were gathered before the Covid-19 pandemic; the implications of this, and of the innovative research design, are discussed.Crown Copyright © 2023. Published by Elsevier Ltd. All rights reserved.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Using speech-to-text technology to empower young writers with special educational needs - ScienceDirect.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {36863156},
}

@InProceedings{lopez2022evaluation,
  author    = {Lopez, Alianda and Liesenfeld, Andreas and Dingemanse, Mark},
  booktitle = {Proceedings of the 18th Conference on Natural Language Processing (KONVENS 2022)},
  date      = {2022},
  title     = {Evaluation of Automatic Speech Recognition for Conversational Speech in Dutch, English and German: What Goes Missing?},
  pages     = {135--143},
  url       = {https://aclanthology.org/2022.konvens-1.16.pdf},
  urldate   = {2024-02-01},
  abstract  = {As voice user interfaces and conversational
agents grow in importance, automatic speech
recognition (ASR) encounters increasingly
free-form and informal input data. Conversational speech is at once the most challenging
and the most ecologically relevant type of data
for speech recognition in this context. Here
we evaluate the performance of several ASR
engines on conversational speech in three languages, focusing on the fate of backchannels
and other interactionally relevant elements of
talk. We propose forms of error analysis based
on ngram salience scoring that can complement
default measures like word error rates (WER)
and are more informative of ASR’s ability to
live up to the task of accurately representing
real-world interaction.},
  year      = {2022},
}

@Article{Kambouri2023,
  author       = {M Kambouri and H Simon and G Brooks},
  date         = {2023},
  journaltitle = {Research in developmental disabilities},
  title        = {Using speech-to-text technology to empower young writers with special educational needs.},
  doi          = {10.1016/j.ridd.2023.104466},
  url          = {https://www.sciencedirect.com/science/article/pii/S0891422223000446?via=ihub},
  urldate      = {2023-10-29},
  abstract     = {This article reports the first group-based intervention study in the UK of using speech to-text (STT) technology to improve the writing of children with special educational needs and disabilities (SEND). Over a period of five years, thirty children took part in total from three settings; a mainstream school, a special school and a special unit of a different mainstream school. All children had Education, Health and Care Plans because of their difficulties in spoken and written communication. Children were trained to use the Dragon STT system, and used it on set tasks for 16-18 weeks. Handwritten text and self-esteem were assessed before and after the intervention, and screen-written text at the end. The results showed that this approach had boosted the quantity and quality of handwritten text, with post-test screen-written text significantly better than handwritten at post-test. The self-esteem instrument also showed positive and statistically significant results. The findings support the feasibility of using STT to support children with writing difficulties. All the data were gathered before the Covid-19 pandemic; the implications of this, and of the innovative research design, are discussed.Crown Copyright © 2023. Published by Elsevier Ltd. All rights reserved.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Using speech-to-text technology to empower young writers with special educational needs - ScienceDirect.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {36863156},
}

@Article{Kambouri2023,
  author       = {M Kambouri and H Simon and G Brooks},
  date         = {2023},
  journaltitle = {Research in developmental disabilities},
  title        = {Using speech-to-text technology to empower young writers with special educational needs.},
  doi          = {10.1016/j.ridd.2023.104466},
  url          = {https://www.sciencedirect.com/science/article/pii/S0891422223000446?via=ihub},
  urldate      = {2023-10-29},
  abstract     = {This article reports the first group-based intervention study in the UK of using speech to-text (STT) technology to improve the writing of children with special educational needs and disabilities (SEND). Over a period of five years, thirty children took part in total from three settings; a mainstream school, a special school and a special unit of a different mainstream school. All children had Education, Health and Care Plans because of their difficulties in spoken and written communication. Children were trained to use the Dragon STT system, and used it on set tasks for 16-18 weeks. Handwritten text and self-esteem were assessed before and after the intervention, and screen-written text at the end. The results showed that this approach had boosted the quantity and quality of handwritten text, with post-test screen-written text significantly better than handwritten at post-test. The self-esteem instrument also showed positive and statistically significant results. The findings support the feasibility of using STT to support children with writing difficulties. All the data were gathered before the Covid-19 pandemic; the implications of this, and of the innovative research design, are discussed.Crown Copyright © 2023. Published by Elsevier Ltd. All rights reserved.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Using speech-to-text technology to empower young writers with special educational needs - ScienceDirect.pdf:PDF},
  mag_id       = {null},
  pmcid        = {null},
  pmid         = {36863156},
}

@Article{barbiers2004reflexieven,
  author    = {Barbiers, Sjef and Bennis, HJ},
  booktitle = {Taeldeman, man van de taal, schatbewaarder van de taal},
  date      = {2004},
  title     = {Reflexieven in dialecten van het Nederlands: chaos of structuur?},
  isbn      = {9038206518},
  pages     = {43--58},
  publisher = {Academia Press en Vakgroep Nederlandse Taalkunde Universiteit Gent},
  url       = {https://pure.knaw.nl/portal/nl/publications/reflexieven-in-dialecten-van-het-nederlands-chaos-of-structuur},
  urldate   = {2024-02-01},
  year      = {2004},
}

@Article{Shakhovska2019a,
  author   = {Shakhovska, Natalya and Basystiuk, Oleh and Shakhovska, Khrystyna},
  date     = {2019-06},
  title    = {Development of the Speech-to-Text Chatbot Interface Based on Google API},
  url      = {https://www.researchgate.net/publication/360054074_Development_of_the_Speech-to-Text_Chatbot_Interface_Based_on_Google_API},
  urldate  = {2024-02-03},
  abstract = {The paper describes possibilities, which are provided by open APIs, and how to use them for creating unified interfaces using the example of our bot based on Google API. In last decade AI technologies became widespread and easy to implement and use. One of the most perspective technology in the AI field is speech recognition as part of natural language processing. New speech recognition technologies and methods will become a central part of future life because they save a lot of communication time, replacing common texting with voice and audio. In addition, this paper explores the advantages and disadvantages of well known chatbots. The method of their improvement is built. The algorithms of Rabin Karp and Knut Pratt are used. The time complexity of the proposed algorithm is compared with existed one.},
}

@Article{ghyselen2020clearing,
  author    = {Ghyselen, Anne-Sophie and Breitbarth, Anne and Farasyn, Melissa and Van Keymeulen, Jacques and van Hessen, Arjan},
  date      = {2020},
  title     = {Clearing the transcription hurdle in dialect corpus building: The corpus of southern Dutch dialects as case study},
  pages     = {10},
  url       = {https://www.frontiersin.org/articles/10.3389/frai.2020.00010/full},
  urldate   = {2024-02-14},
  volume    = {3},
  journal   = {Frontiers in artificial intelligence},
  publisher = {Frontiers Media SA},
  year      = {2020},
}

@Article{li2022recent,
  author    = {Li, Jinyu and others},
  date      = {2022},
  title     = {Recent advances in end-to-end automatic speech recognition},
  number    = {1},
  url       = {https://www.researchgate.net/publication/355872363_Recent_Advances_in_End-to-End_Automatic_Speech_Recognition},
  urldate   = {2024-02-17},
  volume    = {11},
  abstract  = {Recently, the speech community is seeing a significant trend of moving
from deep neural network based hybrid modeling to end to end (E2E)
modeling for automatic speech recognition (ASR). While E2E models
achieve the state of the art results in most benchmarks in terms of ASR
accuracy, hybrid models are still used in a large proportion of commercial
ASR systems at the current time. There are lots of practical factors that
affect the production model deployment decision. Traditional hybrid
models, being optimized for production for decades, are usually good
at these factors. Without providing excellent solutions to all these
factors, it is hard for E2E models to be widely commercialized. In this
paper, we will overview the recent advances in E2E models, focusing on
technologies addressing those challenges from the industry’s perspective.},
  file      = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/recent advances.pdf:PDF},
  journal   = {APSIPA Transactions on Signal and Information Processing},
  priority  = {prio1},
  publisher = {Now Publishers, Inc.},
  ranking   = {rank4},
  year      = {2022},
}

@Article{Wei2022,
  author       = {X. Wei and C. Cucchiarini and R. {van Hout} and H. Strik},
  date         = {2022},
  journaltitle = {Speech Communication},
  title        = {Automatic Speech Recognition and Pronunciation Error Detection of Dutch Non-native Speech: cumulating speech resources in a pluricentric language},
  doi          = {https://doi.org/10.1016/j.specom.2022.08.004},
  issn         = {0167-6393},
  pages        = {1-9},
  url          = {https://www.sciencedirect.com/science/article/pii/S016763932200108X},
  urldate      = {2024-02-17},
  volume       = {144},
  abstract     = {The shortage of large-scale learners’ speech corpora and precise manual annotations are two major challenges for automatic L2 speech recognition and error detection in L2 speech, especially for non-dominant varieties of pluricentric languages. In these cases, collecting and annotating large non-native (L2 learner) corpora for all language varieties is often unattainable. In this study, we investigated ways of addressing these problems through conventional and transfer learning Deep Neural Network (DNN) based Automatic Speech Recognition (ASR) and ASR-based pronunciation error detection (PED) by cumulating Netherlandic Dutch and Flemish Dutch speech resources. First, we show that for ASR the baseline system can be improved by combining the Netherlandic Dutch and Flemish Dutch datasets. Next, through the knowledge learned from models trained on the Netherlandic Dutch data, the Flemish Dutch learners' ASR model can be further improved. In order to evaluate the performance of the PED algorithms in the absence of learner speech data with pronunciation error annotations, we introduced plausible pronunciation errors in the native corpora based on knowledge from Flemish learner speech, in order to simulate non-native speech errors. For PED we found that the results are much better for a GOP classifier trained on Flemish Dutch data than for one trained on Netherlandic Dutch data. PED produced worse results when the Netherlandic Dutch data were merged with the Flemish Dutch data, while for ASR, lower WERs were attained. Whether adding Netherlandic Dutch data to Flemish Dutch data is beneficial, thus seems to depend on the specific task the data are used for. We discuss these results, compare them to those of related research and suggest avenues for future research.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Automatic speech recogn.pdf:PDF},
  keywords     = {Pluricentric languages, Automatic speech recognition, Transfer learning, Pronunciation error detection},
  priority     = {prio1},
  ranking      = {rank5},
}

@Article{VanDyck2021,
  author       = {Van Dyck, Bob and BabaAli, Bagher and Van Compernolle, Dirk},
  date         = {2021-12},
  journaltitle = {Computational Linguistics in the Netherlands Journal},
  title        = {A Hybrid ASR System for Southern Dutch},
  pages        = {27–34},
  url          = {https://clinjournal.org/clinj/article/view/119},
  urldate      = {2024-02-17},
  volume       = {11},
  abstractnote = {&amp;lt;p&amp;gt;Classical hybrid models for automatic speech recognition were recently outperformed by end-toend models on popular benchmarks such as LibriSpeech. However, in many real life situations, hybrid systems can prevail due to independent training, optimization and tuning of the acoustic and language models. In this work, we implemented a state-of-the-art hybrid system for Southern Dutch. For the acoustic model, we train a HMM-DNN on 155 hrs of the Corpus Gesproken Nederlands (CGN) with a rather standard Kaldi recipe. As reference, we reused language models developed during our N-Best 2008 evaluation. We further investigated the effect of language model order and size on WER for a variety of test sets (held out data from CGN, N-Best dev and test sets). Best results, 10.12% WER on the N-Best test set, are obtained with a 400k lexicon and a 4-gram language model (with 231M parameters). This new hybrid system outperforms our older HMM-GMM based N-Best system by over 40%. Pruning away 90% of the LM parameters yields a compact model suitable for small scale real-time apps while only taking a 10% relative hit on performance.&amp;lt;/p&amp;gt;},
  priority     = {prio1},
  ranking      = {rank4},
}

@Article{bentum-etal-2022-speech,
  author    = {Bentum, Martijn and ten Bosch, Louis and van den Heuvel, Henk and Wills, Simone and van der Niet, Domenique and Dijkstra, Jelske and Van de Velde, Hans},
  date      = {2022},
  title     = {A Speech Recognizer for {F}risian/{D}utch Council Meetings},
  editor    = {Calzolari, Nicoletta and B{\'e}chet, Fr{\'e}d{\'e}ric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Odijk, Jan and Piperidis, Stelios},
  pages     = {1009--1015},
  url       = {https://aclanthology.org/2022.lrec-1.107},
  urldate   = {2024-02-17},
  abstract  = {We developed a bilingual Frisian and Dutch speech recognizer for council meetings in Frysl (the Netherlands). During these meetings both Frisian and Dutch are spoken, and code switching between both languages shows up frequently. The new speech recognizer is based on an existing speech recognizer for Frisian and Dutch named FAME!, which was trained and tested on historical radio broadcasts. Adapting a speech recognizer for the council meeting domain is challenging because of acoustic background noise, speaker overlap and the jargon typically used in council meetings. To train the new recognizer, we used the radio broadcast materials utilized for the development of the FAME! recognizer and added newly created manually transcribed audio recordings of council meetings from eleven Frisian municipalities, the Frisian provincial council and the Frisian water board. The council meeting recordings consist of 49 hours of speech, with 26 hours of Frisian speech and 23 hours of Dutch speech. Furthermore, from the same sources, we obtained texts in the domain of council meetings containing 11 million words; 1.1 million Frisian words and 9.9 million Dutch words. We describe the methods used to train the new recognizer, report the observed word error rates, and perform an error analysis on remaining errors.},
  address   = {Marseille, France},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = jun,
  priority  = {prio1},
  publisher = {European Language Resources Association},
  ranking   = {rank4},
  year      = {2022},
}

@Article{Arts2021,
  author       = {Floor Arts and Deniz Başkent and Terrin N. Tamati},
  date         = {2021},
  journaltitle = {Speech Communication},
  title        = {Development and structure of the VariaNTS corpus: A spoken Dutch corpus containing talker and linguistic variability},
  doi          = {https://doi.org/10.1016/j.specom.2020.12.006},
  issn         = {0167-6393},
  pages        = {64-72},
  url          = {https://www.sciencedirect.com/science/article/pii/S0167639320303083},
  urldate      = {2024-02-17},
  volume       = {127},
  abstract     = {Speech perception and spoken word recognition are not only affected by what is being said, but also by who is speaking. Currently, publicly available corpora of spoken Dutch do not offer a wide variety of linguistic materials produced by multiple talkers. The VariaNTS (Variatie in Nederlandse Taal en Sprekers) corpus is a Dutch spoken corpus that was developed to maximize both linguistic and talker variability. It contains 1000 items from 11 linguistic subcategories, recorded by 8 male and 8 female native speakers of standard Dutch. The corpus contains audio recordings, orthographic transcriptions, item-specific details such as word frequencies, neighborhood densities and phonotactic probabilities, and talker details. The VariaNTS corpus aims to provide new materials to be used for broad assessment of speech perception and word recognition in Dutch clinical and academic settings.},
  keywords     = {Spoken Dutch, Speech corpus, Talker variability, Linguistic variability},
  priority     = {prio1},
  ranking      = {rank4},
}

@Article{Pratap2020MLSAL,
  author   = {Vineel Pratap and Qiantong Xu and Anuroop Sriram and Gabriel Synnaeve and Ronan Collobert},
  date     = {2020},
  title    = {MLS: A Large-Scale Multilingual Dataset for Speech Research},
  url      = {https://www.openslr.org/94/},
  urldate  = {2024-02-17},
  volume   = {abs/2012.03411},
  abstract = {Dataset},
  journal  = {ArXiv},
  priority = {prio2},
  year     = {2020},
}

@Article{Feng2024,
  author       = {Siyuan Feng and Bence Mark Halpern and Olya Kudina and Odette Scharenborg},
  date         = {2024},
  journaltitle = {Computer Speech & Language},
  title        = {Towards inclusive automatic speech recognition},
  doi          = {https://doi.org/10.1016/j.csl.2023.101567},
  issn         = {0885-2308},
  pages        = {101567},
  url          = {https://www.sciencedirect.com/science/article/pii/S0885230823000864},
  urldate      = {2024-02-18},
  volume       = {84},
  abstract     = {Practice and recent evidence show that state-of-the-art (SotA) automatic speech recognition (ASR) systems do not perform equally well for all speaker groups. Many factors can cause this bias against different speaker groups. This paper, for the first time, systematically quantifies and finds speech recognition bias against gender, age, regional accents and non-native accents, and investigates the origin of this bias by investigating bias cross-lingually (i.e., Dutch and Mandarin) and for two different SotA ASR architectures (a hybrid DNN-HMM and an attention based end-to-end (E2E) model) through a phoneme error analysis. The results show that only a fraction of the bias can be explained by pronunciation differences between speaker groups, and that in order to mitigate bias, language- and architecture specific solutions need to be found.},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Towards inclusive asr.pdf:PDF},
  keywords     = {Inclusive automatic speech recognition, Bias, Gender, Age, Accent},
  priority     = {prio1},
  ranking      = {rank4},
}

@Article{herygers2023bias,
  author       = {Herygers, Aaricia and Verkhodanova, Vass and Coler, Matt and Scharenborg, Odette and Georges, Munir},
  date         = {2023},
  title        = {Bias in Flemish Automatic Speech Recognition},
  pages        = {158--165},
  url          = {https://www.essv.de/pdf/2023_158_165.pdf},
  urldate      = {2024-02-18},
  booktitle    = {Elektronische Sprachsignalverarbeitung 2023: Tagungsband der 34. Konferenz M{\"u}nchen, 1.-3. M{\"a}rz 2023},
  file         = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/Bias flemish asr.pdf:PDF},
  organization = {TUDpress},
  priority     = {prio1},
  ranking      = {rank4},
  year         = {2023},
}

@Article{cucchiarini-etal-2006-jasmin,
  author    = {Cucchiarini, Catia and Van hamme, Hugo and van Herwijnen, Olga and Smits, Felix},
  date      = {2006},
  title     = {{JASMIN}-{CGN}: Extension of the Spoken {D}utch Corpus with Speech of Elderly People, Children and Non-natives in the Human-Machine Interaction Modality},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
  url       = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/254_pdf.pdf},
  urldate   = {2024-02-18},
  abstract  = {Large speech corpora (LSC) constitute an indispensable resource for conducting research in speech processing and for developing real-life speech applications. In 2004 the Spoken Dutch Corpus (CGN) became available, a corpus of standard Dutch as spoken by adult natives in the Netherlands and Flanders. Owing to budget constraints, CGN does not include speech of children, non-natives, elderly people and recordings of speech produced in human-machine interactions. Since such recordings would be extremely useful for conducting research and for developing HLT applications for these specific groups of speakers of Dutch, a new project, JASMIN-CGN, was started which aims at extending CGN in different ways: by collecting a corpus of contemporary Dutch as spoken by children of different age groups, non-natives with different mother tongues and elderly people in the Netherlands and Flanders and, in addition, by collecting speech material in a communication setting that was not envisaged in CGN: human-machine interaction. We expect that the knowledge gathered from these data can be generalized to developing appropriate systems also for other speaker groups (i.e. adult natives). One third of the data will be collected in Flanders and two thirds in the Netherlands.},
  address   = {Genoa, Italy},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)},
  month     = may,
  priority  = {prio1},
  publisher = {European Language Resources Association (ELRA)},
  year      = {2006},
}

@Article{zhlebinkov2022improving,
  author   = {Zhlebinkov, Nikolay},
  date     = {20022},
  title    = {Improving Northern Regional Dutch Speech Recognition by Adapting Perturbation-based Data Augmentation},
  url      = {https://repository.tudelft.nl/islandora/object/uuid:081e1dc0-6bb3-454c-95cf-b0ac50d7d554?collection=education},
  urldate  = {2024-02-18},
  abstract = {Automatic speech recognition (ASR) does not perform equally well on every speaker. There is bias against many attributes, including accent. To train Dutch ASR, there exists CGN(Corpus Gesproken Nederlands) and as an extension, the JASMIN corpus with annotated accented data. This paper focuses on improving ASR performance for NRAD (Northern regional accented Dutch) speech, training on speakers from the region of Overijssel. To achieve this improvement, the corpus data is augmented using Vocal Tract Length Perturbation (VTLP), which entails randomly warping the frequency of each recording using a factor in the range 0.9, 1.1. The baseline and augmented ASR systems are trained using trigram GMM-HMM (Gaussian mixture model hidden Markov models) through the Kaldi toolkit on the DelftBlue supercomputer. This leads to improvements on word error rates (WER) for all speaker groups and styles, with an overall relative improvement of 14,64 percent and the biggest improvement observed for male speakers - from 25.15 percent WER to 19,68 percent WER. The impact of this augmentation on other accents and non-accented speech is not explored. This experiment can serve as a stepping stone for developing overall more robust and less biased Dutch ASR.},
  file     = {:C\:/Users/jensc/Documents/HGnt_map/Jaar 3/Bachelor Proef/bronnen_files/RP_Paper_NZhlebinkov_v3.5.pdf:PDF},
  year     = {2022},
}

@Comment{jabref-meta: databaseType:biblatex;}
